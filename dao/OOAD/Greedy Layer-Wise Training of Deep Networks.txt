name: Greedy Layer-Wise Training of Deep Networksurl: http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6287632public_in: 《Advances in Neural Information Processing Systems》authors: Schölkopf, B,Platt, J,Hofmann, T,abstract: Complexity theory of circuits strongly suggests that deep architectures can be much more ef cient (sometimes exponentially) than shallow architectures, in terms of computational elements required to represent some functions. Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization appears to often get stuck in poor solutions. Hinton et al. recently introduced a greedy layer-wise unsupervised learning algorithm for Deep Belief Networks (DBN), a generative model with many layers of hidden causal variables. In the context of the above optimization problem, we study this algorithm empirically and explore variants to better understand its success and extend it to cases where the inputs are continuous or where the structure of the input distribution is not revealing enough about the variable to be predicted in a supervised task. Our experiments also confirm the hypothesis that the greedy layer-wise unsupervised training strategy mostly helps the optimization, by initializing weights in a region near a good local minimum, giving rise to internal distributed representations that are high-level abstractions of the input, bringing better generalization.citations_number: 2059Citation: Visualizing and Understanding Convolutional Networks "Matthew D. Zeiler" "Rob Fergus" http://xueshu.baidu.com/usercenter/paper/show?paperid=908cd647dd96ee1d41402d811bf32178Representation Learning: A Review and New Perspectives "Yoshua Bengio" "Aaron Courville" "Pascal Vincent" http://xueshu.baidu.com/usercenter/paper/show?paperid=14655222a985bcb548e882bcd92e006bExtracting and composing robust features with denoising autoencoders "Vincent" "Larochelle" "Hugo" "Yoshua" "Manzagol" "PierreAntoine" http://xueshu.baidu.com/usercenter/paper/show?paperid=4519075f619c1e3b14a964c826b0edb4Multi-column deep neural networks for image classification "Ciregan" "Meier" "Schmidhuber" http://xueshu.baidu.com/usercenter/paper/show?paperid=59dabe4cffd96dc546b42071f86dfa16Building high-level features using large scale unsupervised learning "Quoc V. Le" http://xueshu.baidu.com/usercenter/paper/show?paperid=b1707e4c8a622ca2c7feff7cd37e6162Acoustic Modeling Using Deep Belief Networks "Abdel-rahman Mohamed" "George E. Dahl" "Geoffrey Hinton" http://xueshu.baidu.com/usercenter/paper/show?paperid=db715eae540f4fa01ed7fb0327926d02Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis "Q. V. Le" "W. Y. Zou" "S. Y. Yeung" "A. Y. Ng" http://xueshu.baidu.com/usercenter/paper/show?paperid=c176b3642666514df7c3773d2e435678Unsupervised Learning of Invariant Feature Hierarchies with Applications to Object Recognition "Marc'Aurelio Ranzato" "Fu Jie Huang" "YLan Boureau" "Yann LeCun" http://xueshu.baidu.com/usercenter/paper/show?paperid=b4e14406afad24735ea13ca77d7ee1cfUnsupervised Learning of Invariant Feature Hierarchies with Applications to Object Recognition "Ranzato, M" "Fu Jie Huang" "Boureau, Y.-L" "LeCun, Y" http://xueshu.baidu.com/usercenter/paper/show?paperid=3fba04f455d8ece2ea55db4822027f53References: Training MLPs layer by layer using an objective function for internal representations "Régis Lengellé" "Thierry Denœux" http://xueshu.baidu.com/usercenter/paper/show?paperid=c63f86bfe70b6b48c46afb9a601b16bb