name: Stochastic Dual Coordinate Ascent Methods for Regularized Loss Minimizationurl: http://www.researchgate.net/publication/235357150_Stochastic_Dual_Coordinate_Ascent_Methods_for_Regularized_LossMinimizationpublic_in: ResearchGateauthors: Shalev-Shwartz, Shai,Zhang, Tong,abstract: Abstract:  Stochastic Gradient Descent (SGD) has become popular for solving large scale supervised machine learning optimization problems such as SVM, due to their strong theoretical guarantees. While the closely related Dual Coordinate Ascent (DCA) method has been implemented in various software packages, it has so far lacked good convergence analysis. This paper presents a new analysis of Stochastic Dual Coordinate Ascent (SDCA) showing that this class of methods enjoy strong theoretical guarantees that are comparable or better than SGD. This analysis justifies the effectiveness of SDCA for practical applications.citations_number: 427Citation: Accelerating stochastic gradient descent using predictive variance reduction http://xueshu.baidu.com/usercenter/paper/show?paperid=cc654c840481c7bcd835cc2ef7d0d572SAGA: a fast incremental gradient method with support for non-strongly convex composite objectives http://xueshu.baidu.com/usercenter/paper/show?paperid=3d3d8249db208eac7807753b81c5ea98A Proximal Stochastic Gradient Method with Progressive Variance Reduction http://xueshu.baidu.com/usercenter/paper/show?paperid=0823f25d062984549da18409c30de56bAccelerated proximal stochastic dual coordinate ascent for regularized loss minimization http://xueshu.baidu.com/usercenter/paper/show?paperid=abaeaf9bb6df3112840312bb4cdc9830Coordinate descent algorithms http://xueshu.baidu.com/usercenter/paper/show?paperid=f4f6cc5e1eb0477b084c6ed601b6e978Convex Optimization: Algorithms and Complexity http://xueshu.baidu.com/usercenter/paper/show?paperid=e02919c3d9e48b8c5ffbb628841215a9Optimization Methods for Large-Scale Machine Learning http://xueshu.baidu.com/usercenter/paper/show?paperid=972713604bb4784237ea58a8a34f65b9Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization http://xueshu.baidu.com/usercenter/paper/show?paperid=c095daf83bf58c09f4c825cab051d7a6Accelerated Proximal Stochastic Dual Coordinate Ascent for Regularized Loss Minimization http://xueshu.baidu.com/usercenter/paper/show?paperid=a7377d73b5b548b337b138ba5f34fc12Communication-efficient distributed dual coordinate ascent http://xueshu.baidu.com/usercenter/paper/show?paperid=a0d422332c078a77ed905e9fd19afbebReferences: Training linear SVMs in linear time http://xueshu.baidu.com/usercenter/paper/show?paperid=6564feba203c56d36db06dd733612ce0Pegasos: Primal Estimated sub-GrAdient SOlver for SVM http://xueshu.baidu.com/usercenter/paper/show?paperid=d19c179365153d7743cf5aab07472191The tradeoffs of large scale learning http://xueshu.baidu.com/usercenter/paper/show?paperid=923dd5d3e54e94c69912e8409b63599bSuccessive overrelaxation for support vector machines. http://xueshu.baidu.com/usercenter/paper/show?paperid=c986a2e5dbef193c7cfbc659ee17a867A dual coordinate descent method for large-scale linear SVM http://xueshu.baidu.com/usercenter/paper/show?paperid=54dfcb422ec8bdb4e12fd06ce1a80a70Efficiency of Coordinate Descent Methods on Huge-Scale Optimization Problems http://xueshu.baidu.com/usercenter/paper/show?paperid=97e8135f4f833b552aaa02cf52c020b2Solving large scale linear prediction problems using stochastic gradient descent algorithms http://xueshu.baidu.com/usercenter/paper/show?paperid=1a5ea83cca5a2b2804e1fb583d70df08On the convergence of the coordinate descent method for convex differentiable minimization http://xueshu.baidu.com/usercenter/paper/show?paperid=aaafedaa91fc64a473c98a78ef6f973eA Stochastic Gradient Method with an Exponential Convergence Rate for Strongly-Convex Optimization with Finite Training Sets http://xueshu.baidu.com/usercenter/paper/show?paperid=3cce21f5878971eae5c784c16a097db7Stochastic methods for l1 regularized loss minimization http://xueshu.baidu.com/usercenter/paper/show?paperid=7155146597490282099f5da73ffb134bA Stochastic Approximation Method http://xueshu.baidu.com/usercenter/paper/show?paperid=28d112f12ae1cb04b45767f9f2e2cac0SVM optimization:inverse dependence on training set size http://xueshu.baidu.com/usercenter/paper/show?paperid=413f7baa78c9fef0301993c0b4368f36Exponentiated gradient algorithms for conditional random fields and maxmargin Markov networks http://xueshu.baidu.com/usercenter/paper/show?paperid=677ea3a48a82875075dc2ab414cc652dQP Algorithms with Guaranteed Accuracy and Run Time for Support Vector Machines. http://xueshu.baidu.com/usercenter/paper/show?paperid=f9d8dd201673f2cf11856dcd104de7a5Fast rates for regularized objectives http://xueshu.baidu.com/usercenter/paper/show?paperid=fbe9cfd9cf44390a83b1458cfd078104Fast Rates for Regularized Objectives http://xueshu.baidu.com/usercenter/paper/show?paperid=5b98e956aa2578a5a2eefb40a9ba946dA statistical study of on-line learning http://xueshu.baidu.com/usercenter/paper/show?paperid=09475278968a385ec01ffd719cf653f9