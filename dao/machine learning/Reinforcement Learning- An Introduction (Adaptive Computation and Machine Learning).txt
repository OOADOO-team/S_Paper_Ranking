name: Reinforcement Learning: An Introduction (Adaptive Computation and Machine Learning)url: http://www.emeraldinsight.com/journals.htm?issn=0040-0912&amp;volume=9&amp;articleid=1701182public_in: 《IEEE Transactions on Neural Networks》authors: Richard Sutton,Andrew Barto,abstract: ABSTRACT  Of several responses made to the same situation, those which are accompanied or closely followed by satisfaction to the animal will, other things being equal, be more firmly connected with the situation, so that, when it recurs, they will be more likely to recur; those which are accompanied or closely followed by discomfort to the animal will, other things being equal, have their connections with that situation weakened, so that, when it recurs, they will be less likely to occur. The greater the satisfaction or discomfort, the greater the strengthening or weakening of the bond. (Thorndike, 1911) The idea of learning to make appropriate responses based on reinforcing events has its roots in early psychological theories such as Thorndike&#039;s &quot;law of effect&quot; (quoted above). Although several important contributions were made in the 1950s, 1960s and 1970s by illustrious luminaries such as Bellman, Minsky, Klopf and others (Farley and Clark, 1954; Bellman, 1957; Minsky, 1961; Samuel, 1963; Michie and Chambers, 1968; Grossberg, 1975; Klopf, 1982), the last two decades have wit- nessed perhaps the strongest advances in the mathematical foundations of reinforcement learning, in addition to several impressive demonstrations of the performance of reinforcement learning algo- rithms in real world tasks. The introductory book by Sutton and Barto, two of the most influential and recognized leaders in the field, is therefore both timely and welcome. The book is divided into three parts. In the first part, the authors introduce and elaborate on the es- sential characteristics of the reinforcement learning problem, namely, the problem of learning &quot;poli- cies&quot; or mappings from environmental states to actions so as to maximize the amount of &quot;reward&quot;citations_number: 576Citation: Robot Programming by Demonstration "Sylvain Calinon" http://xueshu.baidu.com/usercenter/paper/show?paperid=1ac58af88ccb02d77e53552886598c08The arcade learning environment: an evaluation platform for general agents "Yavar Naddaf" "Yavar Naddaf" "Joel Veness" "Michael Bowling" http://xueshu.baidu.com/usercenter/paper/show?paperid=1e2681a1ca44076ef8374bdf87fb8ec1The Hidden Information State model: A practical framework for POMDP-based spoken dialogue management "Steve Young" "Simon Keizer" "Jost Schatzmann" "Blaise Thomson" "Kai Yu" http://xueshu.baidu.com/usercenter/paper/show?paperid=73af877af36654e4fb9d0fa47829a596Effective reinforcement learning for mobile robots "Smart, W.D" "Kaelbling, L.P" http://xueshu.baidu.com/usercenter/paper/show?paperid=a820b133440a6085be94ee2f5466b6d1Dynamics of Network Selection in Heterogeneous Wireless Networks: An Evolutionary Game Approach "Dusit Niyato" "Ekram Hossain" http://xueshu.baidu.com/usercenter/paper/show?paperid=c8e57686867b698fa94a920dfeea8fb4Kaelbling (2002), Effective Reinforcement Learning for Mobile Robots "W.D" "Kaelbling, L.P" http://xueshu.baidu.com/usercenter/paper/show?paperid=8c45e997790c5ca5fa05b475b273c46fDecision theory, reinforcement learning, and the brain "Peter Dayan" "Nathaniel D. Daw" http://xueshu.baidu.com/usercenter/paper/show?paperid=c82244ba4cad818838492086e598a06fReinforcement learning: the good, the bad and the ugly. "Dayan P" "Niv Y" http://xueshu.baidu.com/usercenter/paper/show?paperid=1a38abff28b088b5c47970b475bbe5e1What is the function of hippocampal theta rhythm?&mdash;Linking behavioral data to phasic properties of field potential and unit recording data "Hasselmo ME" http://xueshu.baidu.com/usercenter/paper/show?paperid=39cb861d3656dd2b18aebf9438ecfdf0Recommender Systems Research: A Connection-Centric Survey "Perugini, Saverio" "Gonçalves, Marcos André" "Fox, Edward A" http://xueshu.baidu.com/usercenter/paper/show?paperid=b91ba599120e7f684b57d561b70f9e50References: 